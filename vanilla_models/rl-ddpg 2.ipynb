{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DDPG (Deep Deterministic Policy Gradient)\n\nUntil now, we applied [policy gradient (on-policy)](./02-policy-gradient.ipynb) in order to handle continuos spaces. (This method then leads to modern [PPO algorithm](./04-ppo.ipynb).)<br>\nDDPG (Deep Deterministic Policy Gradient) algorithm applies policy gradient and Actor-Critic architecture in deep Q-learning discussed in [DQN](./01-dqn.ipynb). But, as you can see later, it is \"deterministic\" (not \"stochastic\"), and this is then why it's called **off-policy** architecture.\n\nAfter I have introduced DDPG in this notebook, next I'll proceed to more advanced off-policy algorithm, SAC.\n\n*(back to [index](https://github.com/tsmatz/reinforcement-learning-tutorials/))*","metadata":{}},{"cell_type":"code","source":"!pip install torch numpy matplotlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:45.883909Z","iopub.execute_input":"2025-03-26T18:17:45.884257Z","iopub.status.idle":"2025-03-26T18:17:52.865491Z","shell.execute_reply.started":"2025-03-26T18:17:45.884222Z","shell.execute_reply":"2025-03-26T18:17:52.863857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:52.867043Z","iopub.execute_input":"2025-03-26T18:17:52.867531Z","iopub.status.idle":"2025-03-26T18:17:57.570117Z","shell.execute_reply.started":"2025-03-26T18:17:52.867480Z","shell.execute_reply":"2025-03-26T18:17:57.568782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Because DDPG cannot operate discrete action space, and here we then use custom CartPole implementation for continuous action space.<br>\nThe shape of action space in this example is ```Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)```.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nClassic cart-pole system implemented by Rich Sutton et al.\nCopied from http://incompleteideas.net/sutton/book/code/pole.c\npermalink: https://perma.cc/C9ZM-652R\n\"\"\"\n\nimport math\nimport random\n\nclass CartPole():\n    def __init__(self):\n        self._cart_mass = 0.31  # (kg)\n        self._pole_mass = 0.055  # (kg)\n        self._pole_length = 0.4  # (m)\n\n        self.x_threshold = 1.0\n        self.theta_threshold = 12 * 2 * math.pi / 360\n\n        self._state = []\n        self._done = True\n\n    def reset(self):\n        self._step = 0\n        self._cart_position = math.tanh(random.gauss(0.0, 0.01)) * 4.8  # (m)\n        self._cart_velocity = random.uniform(-0.05, 0.05)  # (m/s)\n        initial_pole_angle=random.uniform(-0.05, 0.05)\n        self._pole_angle =  (initial_pole_angle + math.pi) % (2 * math.pi) - math.pi  # (rad)\n        self._pole_angular_velocity = random.uniform(-0.05, 0.05)  # (rad/s)\n\n        # (CartPole-v0 uses numpy.ndarray for state,\n        #  but here returns Python array.)\n        self._state = [self._cart_position, self._cart_velocity, self._pole_angle, self._pole_angular_velocity]\n        self._done = False\n        return self._state\n\n    def step(self, action: float):\n        \"\"\"\n        Args:\n            action: float value between -1.0 and 1.0\n        \"\"\"\n        if self._done:\n            raise Exception(\"Cannot run step() before reset\")\n\n        self._step += 1\n\n        # Add a small random noise\n        # (The agent won't succeed by applying zero force each time.)\n        force = 1.0 * (action + random.uniform(-0.02, 0.02))\n\n        total_mass = self._cart_mass + self._pole_mass\n        pole_half_length = self._pole_length / 2\n        pole_mass_length = self._pole_mass * pole_half_length\n\n        cosTheta = math.cos(self._pole_angle)\n        sinTheta = math.sin(self._pole_angle)\n\n        temp = (\n            force + pole_mass_length * self._pole_angular_velocity ** 2 * sinTheta\n        ) / total_mass\n        angularAccel = (9.8 * sinTheta - cosTheta * temp) / (\n            pole_half_length\n            * (4.0 / 3.0 - (self._pole_mass * cosTheta ** 2) / total_mass)\n        )\n        linearAccel = temp - (pole_mass_length * angularAccel * cosTheta) / total_mass\n\n        self._cart_position = self._cart_position + 0.02 * self._cart_velocity\n        self._cart_velocity = self._cart_velocity + 0.02 * linearAccel\n\n        self._pole_angle = (\n            self._pole_angle + 0.02 * self._pole_angular_velocity\n        )\n        self._pole_angle = (self._pole_angle + math.pi) % (2 * math.pi) - math.pi\n\n        self._pole_angular_velocity = (\n            self._pole_angular_velocity + 0.02 * angularAccel\n        )\n\n        # (CartPole-v0 uses numpy.ndarray for state,\n        #  but here returns Python array.)\n        self._state = [self._cart_position, self._cart_velocity, self._pole_angle, self._pole_angular_velocity]\n        term = self._state[0] < -self.x_threshold or \\\n            self._state[0] > self.x_threshold or \\\n            self._state[2] < -self.theta_threshold or \\\n            self._state[2] > self.theta_threshold\n        term = bool(term)\n        trunc = (self._step == 500)\n        trunc = bool(trunc)\n        self._done = bool(term or trunc)\n        return self._state, 1.0, term, trunc, {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:57.571430Z","iopub.execute_input":"2025-03-26T18:17:57.572119Z","iopub.status.idle":"2025-03-26T18:17:57.588784Z","shell.execute_reply.started":"2025-03-26T18:17:57.572070Z","shell.execute_reply":"2025-03-26T18:17:57.586956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = CartPole()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:57.590273Z","iopub.execute_input":"2025-03-26T18:17:57.590776Z","iopub.status.idle":"2025-03-26T18:17:57.615736Z","shell.execute_reply.started":"2025-03-26T18:17:57.590717Z","shell.execute_reply":"2025-03-26T18:17:57.614400Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First of all, we will start with the following equation (Bellman equation) discussed in [Q-Learning](./00-q-learning.ipynb) :\n\n$$ Q^{*}(s_t,a_t) = r_t + \\gamma \\max_a{Q(s_{t+1},a)} \\;\\;\\;\\;\\;\\; (1)$$\n\nwhere $ Q^{*} $ means the optimal $ Q $ value.\n\nNow we consider Q-network $ Q_{\\phi} $ where $\\phi$ is parameters.<br>\nTo optimize the equation (1), we should find $\\phi$ to minimize the following loss $L$ for tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $.\n\n$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\max_a{Q_{\\phi}(s_{t+1},a)} \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (2)$$\n\nwhere $d_t = 1$ if the episode is done, and $0$ otherwise.\n\n> Note : CartPole returns \"done\" flag as termination (term) or truncation (trunc) separately.<br>\n> When it fails and is terminated, it returns True in termination flag. When it reaches to max 500 actions, it returns True in truncation flag. (See [Readme.md](./Readme.md).)<br>\n> For this reason, we will set $d_t=1$ only when termination is True. (If we set $d_t=1$ in truncation, the q-value in this state will then be underestimated, even though it's successful.)\n\nTo make the above maximization $\\max_a Q_{\\phi}(s_{t+1}, a)$ work in continuous action space, we introduce the action policy $\\mu_{\\theta}(s)$, with which we'll find $\\theta$ to maximize $ Q_{\\phi}(s, \\mu_{\\theta}(s)) $.<br>\nThe equation (2) will then be written as follows.\n\n$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) Q_{\\phi}(s_{t+1},\\mu_{\\theta}(s_{t+1})) \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (3) $$\n\n> Note : As you will find, this will again be based on [Actor-Critic method](./03-actor-critic.ipynb) which separates policy ($\\mu_{\\theta}$) and value evaluation ($Q_{\\phi}$) in algorithm.\n\nPlease take care for the difference between this policy $\\mu_{\\theta}(s)$ and a policy used in [on-policy](./02-policy-gradient.ipynb) method. Here $\\mu_{\\theta}(s)$ is deterministic and it doesn't care how the action space is distributed, unlike $ P(a | \\pi_\\theta (s)) $ in on-policy methods. (This is because DDPG is called \"deterministic\" and it's in off-policy method.)\n\nTo make the minimization (3) stable, we also introduce the separated parameters $\\phi^{\\prime}$ and $\\theta^{\\prime}$ (called **target**) as follows.\n\n$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) Q_{{\\phi}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (4) $$\n\nAnd these parameters are delayed with coefficient parameter (hyper-parameter) $ \\tau $ (which is near 0) after each training.\n\n$ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n\n$ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $\n\n> Note : Remember that we also have only updated the value of current state $ s_t $ (not the value of next state $ s_{t+1} $) in [Q-learning example](./00-q-learning.ipynb).<br>\n> See [Deep Q-Network (DQN) example](./00-q-learning.ipynb) for the reason of separating Q-networks.\n\nTo summarize algorithm :\n\n- Collect tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $\n- Optimize $ \\phi $ to minimize $L$ in (4)\n- Optimize $ \\theta $ to maximize $Q_{\\phi}(s, \\mu_{\\theta}(s))$\n- Update target parameters as follows\n    - $ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n    - $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $\n\nNow let's start building networks.","metadata":{}},{"cell_type":"markdown","source":"(1) Q-network\n\nFirst, we build Q-network (both $Q_{\\phi}(s, a)$ and $Q_{{\\phi}^{\\prime}}(s, a)$) as follows.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass QNet(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super().__init__()\n\n        self.hidden = nn.Linear(5, hidden_dim)\n        self.output = nn.Linear(hidden_dim, 1)\n\n    def forward(self, s, a):\n        outs = torch.concat((s, a), dim=-1)\n        outs = self.hidden(outs)\n        outs = F.relu(outs)\n        outs = self.output(outs)\n        return outs\n\nq_origin_model = QNet().to(device)  # Q_phi\nq_target_model = QNet().to(device)  # Q_phi'\n_ = q_target_model.requires_grad_(False)  # target model doen't need grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:57.617192Z","iopub.execute_input":"2025-03-26T18:17:57.617644Z","iopub.status.idle":"2025-03-26T18:17:57.685644Z","shell.execute_reply.started":"2025-03-26T18:17:57.617568Z","shell.execute_reply":"2025-03-26T18:17:57.684010Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(2) Policy network\n\nNext we build policy network (both $\\mu_{\\theta}(s)$ and $\\mu_{\\theta^{\\prime}}(s)$) as follows.\n\nAs I have mentioned above, action space is ```Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)``` and it's then activated by ```tanh()```.","metadata":{}},{"cell_type":"code","source":"class PolicyNet(nn.Module):\n    def __init__(self, hidden_dim=64):\n        super().__init__()\n\n        self.hidden = nn.Linear(4, hidden_dim)\n        self.output = nn.Linear(hidden_dim, 1)\n\n    def forward(self, s):\n        outs = self.hidden(s)\n        outs = F.relu(outs)\n        outs = self.output(outs)\n        outs = torch.tanh(outs)\n        return outs\n\nmu_origin_model = PolicyNet().to(device)  # mu_theta\nmu_target_model = PolicyNet().to(device)  # mu_theta'\n_ = mu_target_model.requires_grad_(False)  # target model doen't need grad","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:57.688459Z","iopub.execute_input":"2025-03-26T18:17:57.688865Z","iopub.status.idle":"2025-03-26T18:17:57.698535Z","shell.execute_reply.started":"2025-03-26T18:17:57.688829Z","shell.execute_reply":"2025-03-26T18:17:57.697377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(3) Function to optimize network parameters $\\phi, \\theta$\n\nAs I have mentioned above, we optimize parameters as follows :\n\n- Optimize $ \\phi $ to minimize $L$ in (4)\n- Optimize $ \\theta $ to maximize $Q_{\\phi}(s, \\mu_{\\theta}(s))$","metadata":{}},{"cell_type":"code","source":"gamma = 0.99\nopt_q = torch.optim.AdamW(q_origin_model.parameters(), lr=0.0005)\nopt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n\ndef optimize(states, actions, rewards, next_states, dones):\n    # Convert to tensor\n    states = torch.tensor(states, dtype=torch.float).to(device)\n    actions = torch.tensor(actions, dtype=torch.float).to(device)\n    actions = actions.unsqueeze(dim=1)\n    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n    rewards = rewards.unsqueeze(dim=1)\n    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n    dones = torch.tensor(dones, dtype=torch.float).to(device)\n    dones = dones.unsqueeze(dim=1)\n\n    # Optimize critic loss\n    opt_q.zero_grad()\n    q_org = q_origin_model(states, actions)\n    mu_tgt_next = mu_target_model(next_states)\n    q_tgt_next = q_target_model(next_states, mu_tgt_next)\n    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next\n    loss_q = F.mse_loss(\n        q_org,\n        q_tgt,\n        reduction=\"none\")\n    loss_q.sum().backward()\n    opt_q.step()\n\n    # Optimize actor loss\n    opt_mu.zero_grad()\n    mu_org = mu_origin_model(states)\n    for p in q_origin_model.parameters():\n        p.requires_grad = False # disable grad in q_origin_model before computation\n    q_tgt_max = q_origin_model(states, mu_org)\n    (-q_tgt_max).sum().backward()\n    opt_mu.step()\n    for p in q_origin_model.parameters():\n        p.requires_grad = True # enable grad again","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:17:57.700919Z","iopub.execute_input":"2025-03-26T18:17:57.701334Z","iopub.status.idle":"2025-03-26T18:18:00.870883Z","shell.execute_reply.started":"2025-03-26T18:17:57.701299Z","shell.execute_reply":"2025-03-26T18:18:00.869687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(4) Function to update target parameters $\\phi^{\\prime}, \\theta^{\\prime}$\n\nTarget parameters are updated as follows.\n\n- $ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n- $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $","metadata":{}},{"cell_type":"code","source":"tau = 0.002\n\ndef update_target():\n    for var, var_target in zip(q_origin_model.parameters(), q_target_model.parameters()):\n        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n        var_target.data = tau * var.data + (1.0 - tau) * var_target.data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:18:00.872011Z","iopub.execute_input":"2025-03-26T18:18:00.872530Z","iopub.status.idle":"2025-03-26T18:18:00.880580Z","shell.execute_reply.started":"2025-03-26T18:18:00.872499Z","shell.execute_reply":"2025-03-26T18:18:00.878342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(5) Replay buffer\n\nIn on-policy architecture, we fed the sequential samples (trajectory) for each training batch. (See [here](./02-policy-gradient.ipynb).)<br>\nTo prevent from learning only for recent experiences in DDPG, we store the past experience of tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $ in buffer (called \"replay buffer\") and pick up the randomized samples in batch training.","metadata":{}},{"cell_type":"code","source":"class replayBuffer:\n    def __init__(self, buffer_size: int):\n        self.buffer_size = buffer_size\n        self.buffer = []\n\n    def add(self, item):\n        if len(self.buffer) == self.buffer_size:\n            self.buffer.pop(0)\n        self.buffer.append(item)\n\n    def sample(self, batch_size):\n        items = random.sample(self.buffer, batch_size)\n        states   = [i[0] for i in items]\n        actions  = [i[1] for i in items]\n        rewards  = [i[2] for i in items]\n        n_states = [i[3] for i in items]\n        dones    = [i[4] for i in items]\n        return states, actions, rewards, n_states, dones\n\n    def length(self):\n        return len(self.buffer)\n\nbuffer = replayBuffer(buffer_size=20000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:18:00.882213Z","iopub.execute_input":"2025-03-26T18:18:00.882771Z","iopub.status.idle":"2025-03-26T18:18:00.916471Z","shell.execute_reply.started":"2025-03-26T18:18:00.882706Z","shell.execute_reply":"2025-03-26T18:18:00.915087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(6) Noise for exploration\n\nAs I mentioned above, the policy $\\mu_{\\theta}(s)$ is deterministic and it doesn't care how the actions are selected. In order for this reason, the noise will be added for exploration to pick up samples in episode's trials. (If not, this will stuck in the same values forever.)<br>\nIn DDPG, the following noise (so called [Ornstein-Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) noise) is often used.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nOrnstein-Uhlenbeck noise implemented by OpenAI\nCopied from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n\"\"\"\nclass OrnsteinUhlenbeckActionNoise:\n    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.dt = dt\n        self.x0 = x0\n        self.reset()\n\n    def __call__(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n        self.x_prev = x\n        return x\n\n    def reset(self):\n        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n\nou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)\n\n# pick up action with Ornstein-Uhlenbeck noise\ndef pick_sample(s):\n    with torch.no_grad():\n        s = np.array(s)\n        s_batch = np.expand_dims(s, axis=0)\n        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n        action_det = mu_origin_model(s_batch)\n        action_det = action_det.squeeze(dim=1)\n        noise = ou_action_noise()\n        action = action_det.cpu().numpy() + noise\n        action = np.clip(action, -1.0, 1.0)\n        return float(action.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:18:00.917767Z","iopub.execute_input":"2025-03-26T18:18:00.918111Z","iopub.status.idle":"2025-03-26T18:18:00.941486Z","shell.execute_reply.started":"2025-03-26T18:18:00.918084Z","shell.execute_reply":"2025-03-26T18:18:00.940135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(7) Now let's put it all together !\n\nI note that here I train (optimize parameters) in each step, and it might then be slow to step.","metadata":{}},{"cell_type":"code","source":"batch_size = 250\n\nreward_records = []\nfor i in range(10000):\n    # Run episode till done\n    s = env.reset()\n    done = False\n    cum_reward = 0\n    while not done:\n        a = pick_sample(s)\n        s_next, r, term, trunc, _ = env.step(a)\n        done = term or trunc\n        buffer.add([s, a, r, s_next, float(term)])  # (see above note for truncation)\n        cum_reward += r\n\n        # Train (optimize parameters)\n        if buffer.length() >= batch_size:\n            states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n            optimize(states, actions, rewards, n_states, dones)\n            update_target()\n        s = s_next\n\n    # Output total rewards in episode (max 500)\n    print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\r\")\n    reward_records.append(cum_reward)\n\n    # stop if reward mean > 475.0\n    if np.average(reward_records[-50:]) > 475.0:\n        break\n\nprint(\"\\nDone\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T18:18:00.942693Z","iopub.execute_input":"2025-03-26T18:18:00.943163Z","execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Generate recent 50 interval average\naverage_reward = []\nfor idx in range(len(reward_records)):\n    avg_list = np.empty(shape=(1,), dtype=int)\n    if idx < 50:\n        avg_list = reward_records[:idx+1]\n    else:\n        avg_list = reward_records[idx-49:idx+1]\n    average_reward.append(np.average(avg_list))\nplt.plot(reward_records)\nplt.plot(average_reward)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clipped double-Q learning (Twin-Q)\n\nIn advanced algorithms, such as TD3 (Twin Delayed DDPG), 2 Q-networks - $ Q_{\\phi_1}(s, a), Q_{\\phi_2}(s, a) $ - and corresponding 2 target networks - $ Q_{\\phi_1^{\\prime}}(s, a), Q_{\\phi_2^{\\prime}}(s, a) $ - are used to optimize in order to mitigate the risk of overestimation in Q-function.\n\nIn this double-Q algorithms, we optimize parameters as follows :\n\n- Optimize $ \\phi_1 $ to minimize $ E\\left[ \\left( Q_{\\phi_1}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] $\n- Optimize $ \\phi_2 $ to minimize $ E\\left[ \\left( Q_{\\phi_2}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] $\n- Optimize $ \\theta $ to maximize $Q_{\\phi_1}(s, \\mu_{\\theta}(s))$\n\nAnd update target parameters as follows :\n\n- $ \\phi_1^{\\prime} = \\tau \\phi_1 + (1-\\tau) \\phi_1^{\\prime} $\n- $ \\phi_2^{\\prime} = \\tau \\phi_2 + (1-\\tau) \\phi_2^{\\prime} $\n- $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $","metadata":{}},{"cell_type":"markdown","source":"(1) First, regenerate Q-network and policy network as follows.","metadata":{}},{"cell_type":"code","source":"q_origin_model1 = QNet().to(device)  # Q_phi1\nq_origin_model2 = QNet().to(device)  # Q_phi2\nq_target_model1 = QNet().to(device)  # Q_phi1'\nq_target_model2 = QNet().to(device)  # Q_phi2'\n_ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n_ = q_target_model2.requires_grad_(False)  # target model doen't need grad","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mu_origin_model = PolicyNet().to(device)  # mu_theta\nmu_target_model = PolicyNet().to(device)  # mu_theta'\n_ = mu_target_model.requires_grad_(False)  # target model doen't need grad","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(2) Regenerate optimize function.","metadata":{}},{"cell_type":"code","source":"gamma = 0.99\nopt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\nopt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\nopt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n\ndef optimize(states, actions, rewards, next_states, dones):\n    # Convert to tensor\n    states = torch.tensor(states, dtype=torch.float).to(device)\n    actions = torch.tensor(actions, dtype=torch.float).to(device)\n    actions = actions.unsqueeze(dim=1)\n    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n    rewards = rewards.unsqueeze(dim=1)\n    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n    dones = torch.tensor(dones, dtype=torch.float).to(device)\n    dones = dones.unsqueeze(dim=1)\n\n    # Compute r + gamma * (1 - done) * min Q (s_next, mu_target(s_next))\n    mu_tgt_next = mu_target_model(next_states)\n    q1_tgt_next = q_target_model1(next_states, mu_tgt_next)\n    q2_tgt_next = q_target_model2(next_states, mu_tgt_next)\n    q_tgt_next_min = torch.minimum(q1_tgt_next, q2_tgt_next)\n    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next_min\n\n    # Optimize critic loss for Q-network1\n    opt_q1.zero_grad()\n    q1_org = q_origin_model1(states, actions)\n    loss_q1 = F.mse_loss(\n        q1_org,\n        q_tgt,\n        reduction=\"none\")\n    loss_q1.sum().backward()\n    opt_q1.step()\n\n    # Optimize critic loss for Q-network2\n    opt_q2.zero_grad()\n    q2_org = q_origin_model2(states, actions)\n    loss_q2 = F.mse_loss(\n        q2_org,\n        q_tgt,\n        reduction=\"none\")\n    loss_q2.sum().backward()\n    opt_q2.step()\n\n    # Optimize actor loss\n    opt_mu.zero_grad()\n    mu_org = mu_origin_model(states)\n    for p in q_origin_model1.parameters():\n        p.requires_grad = False # disable grad in q_origin_model1 before computation\n    q_tgt_max = q_origin_model1(states, mu_org)\n    (-q_tgt_max).sum().backward()\n    opt_mu.step()\n    for p in q_origin_model1.parameters():\n        p.requires_grad = True # enable grad again","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(3) Regenerate update target's params function.","metadata":{}},{"cell_type":"code","source":"tau = 0.002\n\ndef update_target():\n    for var, var_target in zip(q_origin_model1.parameters(), q_target_model1.parameters()):\n        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n    for var, var_target in zip(q_origin_model2.parameters(), q_target_model2.parameters()):\n        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n        var_target.data = tau * var.data + (1.0 - tau) * var_target.data","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"(4) Let's start training again !","metadata":{}},{"cell_type":"code","source":"# reset noise\nou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)\n\n# reset buffer\nbuffer = replayBuffer(buffer_size=50000)\n\n# start training\nbatch_size = 250\nreward_records = []\nfor i in range(10000):\n    # Run episode till done\n    s = env.reset()\n    done = False\n    cum_reward = 0\n    while not done:\n        a = pick_sample(s)\n        s_next, r, term, trunc, _ = env.step(a)\n        done = term or trunc\n        buffer.add([s, a, r, s_next, float(term)])  # (see above note for truncation)\n        cum_reward += r\n\n        # Train (optimize parameters)\n        if buffer.length() >= batch_size:\n            states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n            optimize(states, actions, rewards, n_states, dones)\n            update_target()\n        s = s_next\n\n    # Output total rewards in episode (max 500)\n    print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\r\")\n    reward_records.append(cum_reward)\n\n    # stop if reward mean > 475.0\n    if np.average(reward_records[-50:]) > 475.0:\n        break\n\nprint(\"\\nDone\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.558Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Generate recent 50 interval average\naverage_reward = []\nfor idx in range(len(reward_records)):\n    avg_list = np.empty(shape=(1,), dtype=int)\n    if idx < 50:\n        avg_list = reward_records[:idx+1]\n    else:\n        avg_list = reward_records[idx-49:idx+1]\n    average_reward.append(np.average(avg_list))\nplt.plot(reward_records)\nplt.plot(average_reward)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-26T18:24:33.558Z"}},"outputs":[],"execution_count":null}]}